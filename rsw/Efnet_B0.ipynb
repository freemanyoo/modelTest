{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMiFBn6bxQ7hnY2hCTZRwLI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"YTM38aVlYJMS"},"outputs":[],"source":["# ==============================================================================\n","# âœ… [1ì°¨ í•™ìŠµ Step 1] í™˜ê²½ ì„¤ì • ë° ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸\n","# ==============================================================================\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import transforms, datasets, models\n","import numpy as np\n","from tqdm.notebook import tqdm\n","import time\n","import glob\n","from collections import Counter\n","from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n","from torch.utils.data.sampler import WeightedRandomSampler\n","import random\n","import copy\n","\n","print(\"âœ… í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ\")\n","\n","# Google Colab í™˜ê²½ì—ì„œ Google Drive ë§ˆìš´íŠ¸\n","try:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    print(\"âœ… Google Drive ë§ˆìš´íŠ¸ ì™„ë£Œ\")\n","except ImportError:\n","    print(\"âš ï¸ Google Colab í™˜ê²½ì´ ì•„ë‹™ë‹ˆë‹¤. ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰í•˜ëŠ” ê²½ìš°, ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸ê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤.\")\n","\n","# ë””ë°”ì´ìŠ¤ ì„¤ì • (GPU ìš°ì„  ì‚¬ìš©)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n","print(f\"âš™ï¸ í˜„ì¬ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")"]},{"cell_type":"code","source":["# ==============================================================================\n","# âœ… [1ì°¨ í•™ìŠµ Step 2] ë°ì´í„° ë¡œë”© ë° ë¶ˆê· í˜• ì²˜ë¦¬ (Class Weight & Oversampling)\n","# ==============================================================================\n","# ğŸš¨ğŸš¨ğŸš¨ ì‚¬ìš©ì ë°ì´í„° í´ë” ê²½ë¡œ ì„¤ì • (Google Drive ê²½ë¡œë¡œ ìˆ˜ì • ì™„ë£Œ) ğŸš¨ğŸš¨ğŸš¨\n","data_dir = \"/content/drive/MyDrive/busanit-251001/2-CNN/class3\"\n","\n","if not os.path.exists(data_dir):\n","    raise FileNotFoundError(f\"âš ï¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}. Google Drive ê²½ë¡œë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n","\n","# EfficientNet B0ë¥¼ ìœ„í•œ ì „ì²˜ë¦¬ ì„¤ì •\n","input_size = 224\n","mean = [0.485, 0.456, 0.406]\n","std = [0.229, 0.224, 0.225]\n","\n","# ë°ì´í„° ì¦ê°• Transform\n","data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.RandomResizedCrop(input_size),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.RandomRotation(15),\n","        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean, std)\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize(input_size),\n","        transforms.CenterCrop(input_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean, std)\n","    ]),\n","}\n","\n","print(\"ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...\")\n","\n","# ì „ì²´ ë°ì´í„°ì…‹ ë¡œë”©\n","full_dataset = datasets.ImageFolder(data_dir)\n","class_names = full_dataset.classes\n","num_classes = len(class_names)\n","\n","# í›ˆë ¨:ê²€ì¦ ë°ì´í„° ë¶„í•  (80:20)\n","train_size = int(0.8 * len(full_dataset))\n","val_size = len(full_dataset) - train_size\n","train_data, val_data = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n","\n","print(f\"âœ… í´ë˜ìŠ¤ ëª©ë¡: {class_names}\")\n","print(f\"ğŸ”¢ ì „ì²´ í´ë˜ìŠ¤ ìˆ˜: {num_classes}\")\n","print(f\"ğŸ–¼ï¸ ì „ì²´ ì´ë¯¸ì§€ ìˆ˜: {len(full_dataset)}\")\n","print(f\"ğŸ“ í›ˆë ¨ ë°ì´í„° í¬ê¸°: {len(train_data)}ê°œ, ê²€ì¦ ë°ì´í„° í¬ê¸°: {len(val_data)}ê°œ\")\n","\n","# Custom Dataset í´ë˜ìŠ¤: ë¶„í• ëœ ë°ì´í„°ì— ì „ì²˜ë¦¬ ì ìš©\n","class CustomDataset(Dataset):\n","    def __init__(self, subset, transform=None):\n","        self.subset = subset\n","        self.transform = transform\n","\n","    def __getitem__(self, index):\n","        original_index = self.subset.indices[index]\n","        x, y = self.subset.dataset[original_index]\n","        if self.transform:\n","            x = self.transform(x)\n","        return x, y\n","\n","    def __len__(self):\n","        return len(self.subset)\n","\n","# ì „ì²˜ë¦¬ ì ìš©ëœ ë°ì´í„°ì…‹\n","image_datasets = {\n","    'train': CustomDataset(train_data, transform=data_transforms['train']),\n","    'val': CustomDataset(val_data, transform=data_transforms['val'])\n","}\n","\n","# ----------------------------------------------------\n","# ğŸš¨ ë¶ˆê· í˜• ì²˜ë¦¬ 1: í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜ ê³„ì‚° (Loss Weighting)\n","# ----------------------------------------------------\n","train_labels = [full_dataset.targets[i] for i in train_data.indices]\n","class_counts = Counter(train_labels)\n","print(f\"ğŸ” í›ˆë ¨ ë°ì´í„° í´ë˜ìŠ¤ë³„ ê°œìˆ˜: {class_counts}\")\n","\n","# ê° í´ë˜ìŠ¤ì˜ ê°€ì¤‘ì¹˜ (ì´ ìƒ˜í”Œ ìˆ˜ / í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜)\n","num_samples = len(train_labels)\n","class_weights = [num_samples / class_counts[i] for i in range(num_classes)]\n","class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n","print(f\"âš–ï¸ ì†ì‹¤ í•¨ìˆ˜ì— ì ìš©ë  í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {class_weights_tensor.cpu().numpy()}\")\n","\n","\n","# ----------------------------------------------------\n","# ğŸš¨ ë¶ˆê· í˜• ì²˜ë¦¬ 2: ì˜¤ë²„ìƒ˜í”Œë§ì„ ìœ„í•œ WeightedRandomSampler ì ìš©\n","# ----------------------------------------------------\n","sample_weights = [class_weights[label] for label in train_labels]\n","sampler = WeightedRandomSampler(\n","    weights=sample_weights,\n","    num_samples=len(sample_weights),\n","    replacement=True\n",")\n","print(\"â™»ï¸ í›ˆë ¨ ë°ì´í„°ì— WeightedRandomSampler (ì˜¤ë²„ìƒ˜í”Œë§) ì ìš© ì™„ë£Œ.\")\n","\n","\n","# ë°ì´í„°ë¡œë” ì„¤ì •\n","batch_size = 32\n","dataloaders_dict = {\n","    'train': DataLoader(image_datasets['train'], batch_size=batch_size, sampler=sampler, num_workers=2),\n","    'val': DataLoader(image_datasets['val'], batch_size=batch_size, shuffle=False, num_workers=2)\n","}"],"metadata":{"id":"dH3R0GeJYWF_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ==============================================================================\n","# âœ… [1ì°¨ í•™ìŠµ Step 3] EfficientNet-B0 ëª¨ë¸ ë¡œë“œ ë° ì „ì´ í•™ìŠµ ì„¤ì •\n","# ==============================================================================\n","print(\"\\n EfficientNet-B0 ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n","model_ft = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n","\n","# íŠ¹ì§• ì¶”ì¶œ ë ˆì´ì–´ ë™ê²° (Freeze)\n","for param in model_ft.parameters():\n","    param.requires_grad = False\n","\n","# ìµœì¢… ë¶„ë¥˜ ë ˆì´ì–´ (Classifier) ë³€ê²½\n","num_ftrs = model_ft.classifier[1].in_features\n","model_ft.classifier[1] = nn.Linear(num_ftrs, num_classes)\n","\n","model_ft = model_ft.to(device)\n","\n","print(\"âœ… ëª¨ë¸ ì„¤ì • ì™„ë£Œ\")\n","print(f\"ìµœì¢… ë¶„ë¥˜ í´ë˜ìŠ¤ ìˆ˜: {num_classes}\")"],"metadata":{"id":"pSA_3i57YYEf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ==============================================================================\n","# âœ… [1ì°¨ í•™ìŠµ Step 4] ëª¨ë¸ í›ˆë ¨ í•¨ìˆ˜ ë° ì‹¤í–‰ (ê³ ê¸‰ ì „ëµ ì ìš©)\n","# ==============================================================================\n","# ğŸš¨ ì†ì‹¤ í•¨ìˆ˜ ì„¤ì • ì‹œ class_weights ì ìš©\n","criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n","# 1ì°¨ í•™ìŠµì€ ë§ˆì§€ë§‰ ë¶„ë¥˜ ë ˆì´ì–´ë§Œ í•™ìŠµ (Freeze í•´ì œëœ íŒŒë¼ë¯¸í„°ë§Œ)\n","optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model_ft.parameters()), lr=0.001)\n","\n","# ----------------------------------------------------\n","# ğŸš¨ ê³ ê¸‰ ë°ì´í„° ì¦ê°•: MixUp êµ¬í˜„ í•¨ìˆ˜\n","# ----------------------------------------------------\n","MIXUP_ALPHA = 0.4\n","\n","def mixup_data(x, y, alpha=1.0, device='cuda'):\n","    if alpha > 0:\n","        lam = np.random.beta(alpha, alpha)\n","    else:\n","        lam = 1\n","    batch_size = x.size()[0]\n","    index = torch.randperm(batch_size).to(device)\n","\n","    mixed_x = lam * x + (1 - lam) * x[index, :]\n","    y_a, y_b = y, y[index]\n","    return mixed_x, y_a, y_b, lam\n","\n","def mixup_criterion(criterion, pred, y_a, y_b, lam):\n","    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n","\n","def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, checkpoint_path='efficientnet_best_balanced.pth'):\n","    since = time.time()\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_val_f1 = -1.0 # ğŸš¨ ìµœì  ëª¨ë¸ ê¸°ì¤€ì„ Macro F1 Scoreë¡œ ì„¤ì •\n","\n","    for epoch in range(num_epochs):\n","        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n","        print('-' * 10)\n","\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()\n","            else:\n","                model.eval()\n","\n","            running_loss = 0.0\n","            all_labels = []\n","            all_preds = []\n","\n","            for inputs, labels in tqdm(dataloaders[phase], desc=f'{phase} Phase'):\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","                optimizer.zero_grad()\n","\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    if phase == 'train':\n","                        # ğŸš¨ 50% í™•ë¥ ë¡œ MixUp ì ìš©\n","                        if random.random() < 0.5:\n","                            mixed_inputs, targets_a, targets_b, lam = mixup_data(inputs, labels, MIXUP_ALPHA, device)\n","                            outputs = model(mixed_inputs)\n","                            loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n","                        else:\n","                            outputs = model(inputs)\n","                            loss = criterion(outputs, labels)\n","                    else: # Validation Phase\n","                        outputs = model(inputs)\n","                        loss = criterion(outputs, labels)\n","\n","                    _, preds = torch.max(outputs, 1)\n","\n","                    # í†µê³„ ê³„ì‚°\n","                    if phase == 'val':\n","                        all_labels.extend(labels.cpu().numpy())\n","                        all_preds.extend(preds.cpu().numpy())\n","\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                running_loss += loss.item() * inputs.size(0)\n","\n","\n","            epoch_loss = running_loss / len(image_datasets[phase])\n","\n","            print(f'{phase} Loss: {epoch_loss:.4f}', end=' ')\n","\n","            if phase == 'val':\n","                # ğŸš¨ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§: Macro F1 Score ë° Accuracy ê³„ì‚°\n","                current_acc = accuracy_score(all_labels, all_preds)\n","                current_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n","\n","                # í´ë˜ìŠ¤ë³„ ì •í™•ë„ ê³„ì‚°\n","                cm = confusion_matrix(all_labels, all_preds)\n","                # zero_division=0 ì²˜ë¦¬: í•´ë‹¹ í´ë˜ìŠ¤ì— ìƒ˜í”Œì´ ì—†ê±°ë‚˜(sum=0) ì˜ˆì¸¡ì´ 0ì¸ ê²½ìš°ë¥¼ ë°©ì§€\n","                with np.errstate(divide='ignore', invalid='ignore'):\n","                    class_accuracy = np.nan_to_num(cm.diagonal() / cm.sum(axis=1))\n","\n","                print(f'Acc: {current_acc:.4f} Macro F1: {current_f1:.4f}')\n","                print('--- í´ë˜ìŠ¤ë³„ ì •í™•ë„ ---')\n","                for i, acc in enumerate(class_accuracy):\n","                    print(f'  {class_names[i]}: {acc:.4f}', end=' ')\n","                print('\\n-----------------------')\n","\n","                # ğŸš¨ ìµœì  ëª¨ë¸ ì €ì¥ ê¸°ì¤€: Macro F1 Scoreê°€ ê°€ì¥ ë†’ì„ ë•Œ ì €ì¥\n","                if current_f1 > best_val_f1:\n","                    best_val_f1 = current_f1\n","                    best_model_wts = copy.deepcopy(model.state_dict())\n","                    torch.save(best_model_wts, checkpoint_path)\n","                    print(f\"â­ ìµœì  ëª¨ë¸ ì €ì¥ë¨: {checkpoint_path} (Macro F1: {best_val_f1:.4f})\")\n","\n","    time_elapsed = time.time() - since\n","    print(f'\\nğŸŒŸ í›ˆë ¨ ì™„ë£Œ! ì´ ì†Œìš” ì‹œê°„: {time_elapsed // 60:.0f}ë¶„ {time_elapsed % 60:.0f}ì´ˆ')\n","    print(f'ğŸ† ìµœì  ê²€ì¦ Macro F1: {best_val_f1:.4f}')\n","\n","    model.load_state_dict(best_model_wts)\n","    return model\n","\n","# í›ˆë ¨ ì‹¤í–‰\n","num_epochs = 50\n","model_ft = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, checkpoint_path='efficientnet_best_balanced.pth')"],"metadata":{"id":"LxO4VzgVYbJx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ==============================================================================\n","# âœ… [1ì°¨ í•™ìŠµ Step 5] ìµœì¢… ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥ (ì„ íƒ ì‚¬í•­)\n","# ==============================================================================\n","final_model_path = 'efficientnet_final_balanced.pth'\n","torch.save(model_ft.state_dict(), final_model_path)\n","print(f\"âœ… ìµœì¢… ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥ ì™„ë£Œ: {final_model_path}\")"],"metadata":{"id":"BH107aaHYgdo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ==============================================================================\n","# ğŸ§ª ëª¨ë¸ ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜ (1ì°¨ í•™ìŠµ í›„ í…ŒìŠ¤íŠ¸ ì‹œ ì¬ì‚¬ìš©)\n","# ==============================================================================\n","\n","def load_model_for_prediction(num_classes, model_path, device=device):\n","    # ëª¨ë¸ ì•„í‚¤í…ì²˜ ë‹¤ì‹œ ë¡œë“œ\n","    model = models.efficientnet_b0(weights=None)\n","    num_ftrs = model.classifier[1].in_features\n","    model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n","\n","    # ì €ì¥ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ\n","    try:\n","        model.load_state_dict(torch.load(model_path, map_location=device))\n","        model.to(device)\n","        model.eval()\n","        print(f\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}\")\n","    except Exception as e:\n","        print(f\"âŒ ê°€ì¤‘ì¹˜ íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œ ë˜ëŠ” íŒŒì¼ëª…({model_path})ì„ í™•ì¸í•˜ì„¸ìš”. ì˜¤ë¥˜: {e}\")\n","        return None\n","    return model\n","\n","def preprocess_and_predict(image_path, model, transform, device):\n","    try:\n","        image = Image.open(image_path).convert('RGB')\n","    except Exception as e:\n","        print(f\"âš ï¸  ì´ë¯¸ì§€ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({image_path}): {e}\")\n","        return -1, 0.0\n","\n","    input_tensor = transform(image)\n","    input_batch = input_tensor.unsqueeze(0).to(device)\n","\n","    with torch.no_grad():\n","        output = model(input_batch)\n","        probabilities = torch.nn.functional.softmax(output, dim=1)[0]\n","        confidence, predicted_class = torch.max(probabilities, 0)\n","\n","    return predicted_class.item(), confidence.item()\n","\n","# ì˜ˆì¸¡ìš© ì „ì²˜ë¦¬ (ê²€ì¦ ë°ì´í„°ì™€ ë™ì¼)\n","predict_transforms = transforms.Compose([\n","    transforms.Resize(input_size),\n","    transforms.CenterCrop(input_size),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean, std)\n","])"],"metadata":{"id":"hmGOEdfpYlTX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ==============================================================================\n","# ğŸ§ª [1ì°¨ í•™ìŠµ í›„ í…ŒìŠ¤íŠ¸] - 1ì°¨ í•™ìŠµ ìµœì  ëª¨ë¸ í…ŒìŠ¤íŠ¸\n","# ==============================================================================\n","# ğŸš¨ğŸš¨ğŸš¨ ì˜ˆì¸¡í•  ë‹¨ì¼ ì´ë¯¸ì§€ íŒŒì¼ì˜ ì •í™•í•œ ê²½ë¡œë¥¼ ì—¬ê¸°ì— ì…ë ¥í•˜ì„¸ìš”! ğŸš¨ğŸš¨ğŸš¨\n","test_image_path_1st = \"/content/drive/MyDrive/busanit-251001/2-CNN/í›„ë¼ì´ë“œ1.jpg\" # <--- ê²½ë¡œ ìˆ˜ì • í•„ìš”\n","\n","print(\"\\n\"+\"=\"*80)\n","print(f\"ğŸ§ª 1ì°¨ í•™ìŠµ ìµœì  ëª¨ë¸ í…ŒìŠ¤íŠ¸: {os.path.basename(test_image_path_1st)}\")\n","print(\"=\"*80)\n","\n","loaded_model_1st = load_model_for_prediction(num_classes, model_path='efficientnet_best_balanced.pth')\n","\n","if loaded_model_1st:\n","    predicted_class, confidence = preprocess_and_predict(test_image_path_1st, loaded_model_1st, predict_transforms, device)\n","\n","    if predicted_class != -1:\n","         pred_class = class_names[predicted_class]\n","         conf_percent = confidence * 100\n","         print(f\"âœ… ì˜ˆì¸¡ í´ë˜ìŠ¤: {pred_class}\")\n","         print(f\"âœ… ì‹ ë¢°ë„: {conf_percent:.2f}%\")\n","    else:\n","         print(\"âš ï¸ ì˜ˆì¸¡ í•¨ìˆ˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\")\n","print(\"=\"*80)"],"metadata":{"id":"w3FcEE0tYlgy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ==============================================================================\n","# ğŸ§ª 2ì°¨ í•™ìŠµ: Fine-tuning ì¤€ë¹„\n","# ==============================================================================\n","# ğŸš¨ 1ì°¨ í•™ìŠµ ì¢…ë£Œ í›„ ì´ ì…€ë¶€í„° ì´ì–´ì„œ ì‹¤í–‰\n","# ----------------------------------------------------\n","\n","# âœ… [2ì°¨ Fine-tuning Step 1] ëª¨ë¸ ë™ê²° í•´ì œ ë° í•™ìŠµë¥  ì„¤ì •\n","print(\"\\nâœ¨ Fine-tuning ì¤€ë¹„: ì „ì²´ ëª¨ë¸ ê³„ì¸µ ë™ê²° í•´ì œ\")\n","\n","# 1ì°¨ í•™ìŠµì—ì„œ ë™ê²°í–ˆë˜ ì „ì²´ íŒŒë¼ë¯¸í„° í•´ì œ\n","for param in model_ft.parameters():\n","    param.requires_grad = True\n","\n","# ì „ì²´ íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ ìµœì í™” í•¨ìˆ˜ ì¬ì„¤ì •\n","# ë¯¸ì„¸ ì¡°ì •ì„ ìœ„í•´ 1ì°¨ í•™ìŠµë³´ë‹¤ í›¨ì”¬ ë‚®ì€ í•™ìŠµë¥ (0.00001)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n","optimizer_ft_fine = optim.Adam(model_ft.parameters(), lr=0.00001)\n","\n","# ğŸš¨ 2ì°¨ í•™ìŠµì—ì„œë„ ë¶ˆê· í˜• í•´ì†Œë¥¼ ìœ„í•´ Class Weightsë¥¼ ë‹¤ì‹œ ì ìš©í•©ë‹ˆë‹¤.\n","criterion_fine = nn.CrossEntropyLoss(weight=class_weights_tensor)\n","\n","\n","print(\"ìµœì´ˆ 10ê°œ ê³„ì¸µì˜ require_grad ìƒíƒœ í™•ì¸:\")\n","for name, param in list(model_ft.named_parameters())[:10]:\n","    print(f\"{name}: requires_grad={param.requires_grad}\")\n","\n","print(\"âœ… 2ì°¨ Fine-tuning ì¤€ë¹„ ì™„ë£Œ. ëª¨ë“  ê³„ì¸µì´ í•™ìŠµë©ë‹ˆë‹¤. (Class Weights ì ìš©)\")\n","\n","\n","# âœ… [2ì°¨ Fine-tuning Step 2] 30 Epoch ì¶”ê°€ í•™ìŠµ ì‹¤í–‰\n","print(\"\\n\" + \"=\"*80)\n","print(\"ğŸš€ 2ì°¨ Fine-tuning (ì¶”ê°€ í•™ìŠµ) ì‹œì‘\")\n","print(\"=\"*80)\n","\n","try:\n","    # 1ì°¨ í•™ìŠµ ì‹œ ì €ì¥ëœ ìµœì ì˜ ê· í˜• ì¡íŒ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n","    model_ft.load_state_dict(torch.load('efficientnet_best_balanced.pth', map_location=device))\n","    print(\"âœ… 1ì°¨ í•™ìŠµì—ì„œ ì €ì¥ëœ ìµœì  ê°€ì¤‘ì¹˜ë¡œ Fine-tuning ì‹œì‘!\")\n","except FileNotFoundError:\n","    print(\"âš ï¸ 1ì°¨ í•™ìŠµ íŒŒì¼(efficientnet_best_balanced.pth)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í˜„ì¬ ëª¨ë¸ ìƒíƒœë¡œ Fine-tuningì„ ì§„í–‰í•©ë‹ˆë‹¤.\")\n","\n","num_epochs_fine = 30 # ì¶”ê°€ 30íšŒ í›ˆë ¨\n","\n","model_ft_fine_tuned = train_model(\n","    model_ft,\n","    dataloaders_dict,\n","    criterion_fine,        # ğŸš¨ ìˆ˜ì •ëœ ì†ì‹¤ í•¨ìˆ˜ ì‚¬ìš©\n","    optimizer_ft_fine,     # ğŸš¨ ìˆ˜ì •ëœ ìµœì í™” í•¨ìˆ˜ ì‚¬ìš©\n","    num_epochs=num_epochs_fine,\n","    checkpoint_path='efficientnet_fine_tuned_best_balanced.pth' # 2ì°¨ í•™ìŠµ ì „ìš© íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥\n",")\n","\n","print(\"âœ… 2ì°¨ Fine-tuning ì™„ë£Œ\")"],"metadata":{"id":"75AiUysCYtjY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ==============================================================================\n","# ğŸ§ª [2ì°¨ Fine-tuning í›„ í…ŒìŠ¤íŠ¸] - 2ì°¨ í•™ìŠµ ìµœì  ëª¨ë¸ í…ŒìŠ¤íŠ¸\n","# ==============================================================================\n","# ğŸš¨ğŸš¨ğŸš¨ ì˜ˆì¸¡í•  ë‹¨ì¼ ì´ë¯¸ì§€ íŒŒì¼ì˜ ì •í™•í•œ ê²½ë¡œë¥¼ ì—¬ê¸°ì— ì…ë ¥í•˜ì„¸ìš”! ğŸš¨ğŸš¨ğŸš¨\n","test_image_path_2nd = \"/content/drive/MyDrive/busanit-251001/2-CNN/í›„ë¼ì´ë“œ1.jpg\" # <--- ê²½ë¡œ ìˆ˜ì • í•„ìš”\n","\n","print(\"\\n\"+\"=\"*80)\n","print(f\"ğŸ† 2ì°¨ Fine-tuned ëª¨ë¸ ìµœì¢… í…ŒìŠ¤íŠ¸: {os.path.basename(test_image_path_2nd)}\")\n","print(\"=\"*80)\n","\n","# 2ì°¨ í•™ìŠµì—ì„œ ì €ì¥ëœ ìµœì ì˜ ê°€ì¤‘ì¹˜ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.\n","loaded_model_2nd = load_model_for_prediction(num_classes, model_path='efficientnet_fine_tuned_best_balanced.pth')\n","\n","if loaded_model_2nd:\n","    predicted_class_2nd, confidence_2nd = preprocess_and_predict(test_image_path_2nd, loaded_model_2nd, predict_transforms, device)\n","\n","    if predicted_class_2nd != -1:\n","         pred_class_2nd = class_names[predicted_class_2nd]\n","         conf_percent_2nd = confidence_2nd * 100\n","         print(f\"âœ… ì˜ˆì¸¡ í´ë˜ìŠ¤: {pred_class_2nd}\")\n","         print(f\"âœ… ì‹ ë¢°ë„: {conf_percent_2nd:.2f}%\")\n","    else:\n","         print(\"âš ï¸ ì˜ˆì¸¡ í•¨ìˆ˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\")\n","print(\"=\"*80)"],"metadata":{"id":"KEJvEsPPYxid"},"execution_count":null,"outputs":[]}]}